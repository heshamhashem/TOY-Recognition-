{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An End to End Toy Recognition  (between kiki and miki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1: Build  Image Data  for at least 2 people and Name Them with their Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here \n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def takephoto( ):\n",
    "    objname =input(\"please input object name\")\n",
    "   \n",
    "    cap =cv2.VideoCapture(0)\n",
    "    os.mkdir(objname)\n",
    "    i = 0\n",
    "    while(1):\n",
    "        ret,frame = cap.read()  \n",
    "        cv2.imshow('Live',frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('g'):  #\" Press g to save the image then q to close\"\n",
    "            cv2.imwrite(objname+'/'+objname+str(i)+'.jpg',frame)\n",
    "            i +=1\n",
    "            print(i)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break                                                                          \n",
    "    cap.release()                             \n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input object namemiky\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "takephoto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input object namekiky\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "takephoto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Load The Data , Normalize them and resize them to (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('D:/courses/Electro Pi/Computer vision/high level/2-cnn/Dataset_TOY/Train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('D:/courses/Electro Pi/Computer vision/high level/2-cnn/Dataset_TOY/Test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3: Create Labels list  for example  [0,0,0,0,0,....,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "filenames = os.listdir(\"D:/courses/Electro Pi/Computer vision/high level/2-cnn/Dataset1/Train\")\n",
    "categories = []\n",
    "for filename in filenames:\n",
    "    if filename.startswith('m'):\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kiky0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kiky1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kiky10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kiky11.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kiky12.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename  category\n",
       "0   kiky0.jpg         0\n",
       "1   kiky1.jpg         0\n",
       "2  kiky10.jpg         0\n",
       "3  kiky11.jpg         0\n",
       "4  kiky12.jpg         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #for display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  200 non-null    object\n",
      " 1   category  200 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 4: Check if both classes are same count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2963f35d910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['category'].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 5: Build Your Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense ,Conv2D,MaxPooling2D,Flatten\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(256, (3, 3), input_shape = (64,64,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(128, (4, 4), activation = 'relu'))\n",
    "# Pooling\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 6: Compile , Train , Evaluate your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 256)       7168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 128)       524416    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               2508900   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,040,585\n",
      "Trainable params: 3,040,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 100, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HASHOMA\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5/5 [==============================] - 10s 2s/step - loss: 1.0178 - accuracy: 0.5896 - val_loss: 0.6910 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.7006 - accuracy: 0.4762 - val_loss: 0.6844 - val_accuracy: 0.6000\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6754 - accuracy: 0.5318 - val_loss: 0.6493 - val_accuracy: 0.8500\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6481 - accuracy: 0.6511 - val_loss: 0.5740 - val_accuracy: 0.6500\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5133 - accuracy: 0.7658 - val_loss: 0.3295 - val_accuracy: 0.9500\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3402 - accuracy: 0.9263 - val_loss: 0.2744 - val_accuracy: 0.9000\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3427 - accuracy: 0.8854 - val_loss: 0.2003 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1537 - accuracy: 0.9721 - val_loss: 0.1708 - val_accuracy: 0.9500\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.2066 - accuracy: 0.9019 - val_loss: 0.0976 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1250 - accuracy: 0.9560 - val_loss: 0.0920 - val_accuracy: 0.9500\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0533 - accuracy: 0.9921 - val_loss: 0.0646 - val_accuracy: 0.9500\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0606 - accuracy: 0.9876 - val_loss: 0.0688 - val_accuracy: 0.9500\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0460 - accuracy: 0.9770 - val_loss: 0.1133 - val_accuracy: 0.9500\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0504 - accuracy: 0.9787 - val_loss: 0.0824 - val_accuracy: 0.9500\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0337 - accuracy: 0.9963 - val_loss: 0.0457 - val_accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2963f7e8f10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 15,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.save of <tensorflow.python.keras.engine.sequential.Sequential object at 0x000002963F469F70>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 7: Inference Stage Let it be test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=64x64 at 0x2963F64F430>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('D:/courses/Electro Pi/Computer vision/high level/2-cnn/Dataset1/Train/miky1.jpg', target_size = (64, 64))\n",
    "print(test_image)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = '1'\n",
    "else:\n",
    "    prediction = '2'\n",
    "    \n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
